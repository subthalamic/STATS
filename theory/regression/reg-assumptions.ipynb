{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a043f3e5-e82e-421e-b9ad-220910e8b38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats as st\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0505dc46-2619-4dff-88b0-0632b5ba5856",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = pd.read_csv(\"measurements.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86670124-6ae1-456f-92c3-5696d8bd3d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'group', 'sex', 'birth_year', 'chol_init', 'chol_final',\n",
       "       'trig_init', 'trig_final', 'weight_init', 'weight_final', 'weight_diff',\n",
       "       'height_init', 'height_final', 'height_diff', 'armspan_init',\n",
       "       'armspan_final', 'armspan_diff', 'arm_perimeter_init',\n",
       "       'arm_periemter_final', 'arm_perimeter_diff', 'thorax_perimeter_init',\n",
       "       'thorax_perimeter_final', 'thorax_perimeter_diff',\n",
       "       'abdominal_perimeter_init', 'abdominal_perimeter_final',\n",
       "       'abdominal_perimeter_diff', 'hip_perimeter_init', 'hip_perimeter_final',\n",
       "       'hip_perimeter_diff', 'BMI_init', 'BMI_final', 'BMI_diff',\n",
       "       'tricipital_fold_final', 'tricipital_fold_init', 'abdominal_fold_final',\n",
       "       'abdominal_fold_init', 'subscapular_fold_final',\n",
       "       'subscapular_fold_init'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6ab715-1f99-4394-9b28-a839aa569080",
   "metadata": {},
   "source": [
    "Regression analysis relies on several assumptions for its validity. These assumptions include:\n",
    "\n",
    "1. **Linearity**: There should be a linear relationship between the independent variables (predictors) and the dependent variable (outcome). This means that changes in the independent variables should result in proportional changes in the dependent variable.\n",
    "\n",
    "2. **Homoscedasticity**: Also known as homogeneity of variance, this assumption states that the variance of the residuals should be constant across all levels of the predictors. In other words, the spread of residuals should remain consistent throughout the range of the predictors.\n",
    "\n",
    "3. **Normality of residuals**: The residuals should be normally distributed. This assumption is necessary for the validity of hypothesis tests and confidence intervals based on the regression coefficients. However, violation of normality assumption may not be critical for large sample sizes due to the central limit theorem.\n",
    "\n",
    "4. **No perfect multicollinearity**: There should not be perfect linear relationships among the independent variables. This means that one independent variable should not be a perfect linear combination of the others. High multicollinearity can inflate standard errors and lead to unstable estimates.\n",
    "\n",
    "5. **No influential outliers**: Outliers or influential data points should not excessively influence the regression results. Outliers can distort parameter estimates and affect the overall fit of the model.\n",
    "\n",
    "6. **Independence of residuals**: The residuals (the differences between observed and predicted values) should be independent of each other. This assumption is similar to the independence assumption in correlation tests.\n",
    "\n",
    "\n",
    "When performing regression analysis on time series data, additional assumptions need to be considered to ensure the validity of the regression model. These assumptions include:\n",
    "\n",
    "1. **Stationarity**: The time series data should be stationary, meaning that the statistical properties of the data (such as mean and variance) should remain constant over time. Non-stationarity can lead to spurious regression results and inaccurate parameter estimates.\n",
    "\n",
    "2. **Absence of seasonality**: If the time series data exhibit seasonal patterns, appropriate seasonal adjustments should be made to account for them. Failure to address seasonality can lead to biased estimates and incorrect inferences.\n",
    "\n",
    "3. **Autocorrelation (Serial correlation)**: The residuals (errors) of the regression model should not be correlated with each other over time. Autocorrelation indicates that there is some systematic pattern in the residuals that the model has not captured. This assumption is particularly important in time series data, where observations are often correlated with their past values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68db4bca-7e05-4898-a1a1-4904a490bb38",
   "metadata": {},
   "source": [
    "The Omnibus, Durbin-Watson, and Jarque-Bera statistics are commonly used diagnostics for regression models:\n",
    "\n",
    "1. Omnibus Test:\n",
    "The Omnibus test is a test of the overall significance of a regression model. It tests whether the model as a whole explains a significant amount of variance in the dependent variable. The Omnibus test statistic follows an F-distribution and is calculated by comparing the residual sum of squares of the current model to that of a model with no predictors.\n",
    "\n",
    "2. Durbin-Watson Test:\n",
    "The Durbin-Watson test is a test for autocorrelation in the residuals of a regression model. It assesses whether there is serial correlation between consecutive residuals. The Durbin-Watson statistic ranges from 0 to 4, with a value close to 2 indicating no significant autocorrelation. Values significantly below 2 indicate positive autocorrelation, while values significantly above 2 indicate negative autocorrelation.\n",
    "\n",
    "3. Jarque-Bera Test:\n",
    "The Jarque-Bera test is a test for normality of the residuals in a regression model. It tests whether the residuals are normally distributed. The Jarque-Bera statistic is calculated based on skewness and kurtosis of the residuals. Under the null hypothesis of normality, the Jarque-Bera statistic follows a chi-squared distribution with 2 degrees of freedom.\n",
    "\n",
    "4. The condition number is a measure of how sensitive a function (or system of equations) is to changes or errors in its input. In the context of regression analysis, the condition number is often used to assess multicollinearity among predictor variables. A high condition number indicates that the matrix of predictor variables is ill-conditioned, meaning that small changes in the data could lead to large changes in the estimated coefficients. In practice, it's common to use the singular value decomposition (SVD) of the predictor variable matrix to compute the condition number. The condition number is a useful diagnostic tool for identifying potential issues with multicollinearity in regression models. A condition number much greater than 1 indicates multicollinearity may be present, potentially affecting the stability and interpretability of the regression coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23741969-cf37-4b08-968b-32ba09facab0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvstats",
   "language": "python",
   "name": "venvstats"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
